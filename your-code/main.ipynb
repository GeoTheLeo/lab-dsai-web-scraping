{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping Lab\n",
    "\n",
    "You will find in this notebook some scrapy exercises to practise your scraping skills.\n",
    "\n",
    "**Tips:**\n",
    "\n",
    "- Check the response status code for each request to ensure you have obtained the intended content.\n",
    "- Print the response text in each request to understand the kind of info you are getting and its format.\n",
    "- Check for patterns in the response text to extract the data/info requested in each question.\n",
    "- Visit the urls below and take a look at their source code through Chrome DevTools. You'll need to identify the html tags, special class names, etc used in the html content you are expected to extract.\n",
    "\n",
    "**Resources**:\n",
    "- [Requests library](http://docs.python-requests.org/en/master/#the-user-guide)\n",
    "- [Beautiful Soup Doc](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "- [Urllib](https://docs.python.org/3/library/urllib.html#module-urllib)\n",
    "- [re lib](https://docs.python.org/3/library/re.html)\n",
    "- [lxml lib](https://lxml.de/)\n",
    "- [Scrapy](https://scrapy.org/)\n",
    "- [List of HTTP status codes](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes)\n",
    "- [HTML basics](http://www.simplehtmlguide.com/cheatsheet.php)\n",
    "- [CSS basics](https://www.cssbasics.com/#page_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below are the libraries and modules you may need. `requests`,  `BeautifulSoup` and `pandas` are already imported for you. If you prefer to use additional libraries feel free to do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download, parse (using BeautifulSoup), and print the content from the Trending Developers page from GitHub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://github.com/trending/developers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ibhagwan → https://github.com/ibhagwan\n",
      "Xunzhuo → https://github.com/Xunzhuo\n",
      "Elie Steinbock → https://github.com/elie222\n",
      "Daniel Öster → https://github.com/dalathegreat\n",
      "Weblate (bot) → https://github.com/weblate\n",
      "thinkasany → https://github.com/thinkasany\n",
      "Karl Seguin → https://github.com/karlseguin\n",
      "Jeremiah Lowin → https://github.com/jlowin\n",
      "Henrik Rydgård → https://github.com/hrydgard\n",
      "Tom Moor → https://github.com/tommoor\n",
      "Danny Mösch → https://github.com/SimplyDanny\n",
      "Luis M. Gallardo D. → https://github.com/lgallard\n",
      "Liran Tal → https://github.com/lirantal\n",
      "Jerry Zhao → https://github.com/jerryz123\n",
      "Aleksandr Statciuk → https://github.com/freearhey\n",
      "Sebastian Raschka → https://github.com/rasbt\n",
      "Eric Buehler → https://github.com/EricLBuehler\n",
      "Yorukot → https://github.com/yorukot\n",
      "Derrick Hammer → https://github.com/pcfreak30\n",
      "Yeuoly → https://github.com/Yeuoly\n",
      "Folke Lemaitre → https://github.com/folke\n",
      "AMIT SHEKHAR → https://github.com/amitshekhariitbhu\n",
      "Thomas Schmelzer → https://github.com/tschm\n",
      "Wei → https://github.com/jwcesign\n",
      "comfyanonymous → https://github.com/comfyanonymous\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL of GitHub Trending Developers\n",
    "url = \"https://github.com/trending/developers\"\n",
    "\n",
    "# Send HTTP GET request\n",
    "response = requests.get(url)\n",
    "if response.status_code != 200:\n",
    "    print(\"Failed to fetch page:\", response.status_code)\n",
    "    exit()\n",
    "\n",
    "# Parse HTML with BeautifulSoup\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# Each developer entry is inside <article class=\"Box-row\">\n",
    "developers = soup.find_all(\"article\", class_=\"Box-row\")\n",
    "\n",
    "for dev in developers:\n",
    "    # Developer name (username is usually inside 'h1' > 'a')\n",
    "    name_tag = dev.find(\"h1\", class_=\"h3 lh-condensed\")\n",
    "    if name_tag:\n",
    "        username = name_tag.get_text(strip=True)\n",
    "        profile_url = \"https://github.com\" + name_tag.find(\"a\")[\"href\"]\n",
    "        print(f\"{username} → {profile_url}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Display the names of the trending developers retrieved in the previous step.\n",
    "\n",
    "Your output should be a Python list of developer names. Each name should not contain any html tag.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Find out the html tag and class names used for the developer names. You can achieve this using Chrome DevTools or clicking in 'Inspect' on any browser. Here is an example:\n",
    "\n",
    "![title](example_1.png)\n",
    "\n",
    "2. Use BeautifulSoup `find_all()` to extract all the html elements that contain the developer names. Hint: pass in the `attrs` parameter to specify the class.\n",
    "\n",
    "3. Loop through the elements found and get the text for each of them.\n",
    "\n",
    "4. While you are at it, use string manipulation techniques to replace whitespaces and linebreaks (i.e. `\\n`) in the *text* of each html element. Use a list to store the clean names. Hint: you may also use `.get_text()` instead of `.text` and pass in the desired parameters to do some string manipulation (check the documentation).\n",
    "\n",
    "5. Print the list of names.\n",
    "\n",
    "Your output should look like below:\n",
    "\n",
    "```\n",
    "['trimstray (@trimstray)',\n",
    " 'joewalnes (JoeWalnes)',\n",
    " 'charlax (Charles-AxelDein)',\n",
    " 'ForrestKnight (ForrestKnight)',\n",
    " 'revery-ui (revery-ui)',\n",
    " 'alibaba (Alibaba)',\n",
    " 'Microsoft (Microsoft)',\n",
    " 'github (GitHub)',\n",
    " 'facebook (Facebook)',\n",
    " 'boazsegev (Bo)',\n",
    " 'google (Google)',\n",
    " 'cloudfetch',\n",
    " 'sindresorhus (SindreSorhus)',\n",
    " 'tensorflow',\n",
    " 'apache (TheApacheSoftwareFoundation)',\n",
    " 'DevonCrawford (DevonCrawford)',\n",
    " 'ARMmbed (ArmMbed)',\n",
    " 'vuejs (vuejs)',\n",
    " 'fastai (fast.ai)',\n",
    " 'QiShaoXuan (Qi)',\n",
    " 'joelparkerhenderson (JoelParkerHenderson)',\n",
    " 'torvalds (LinusTorvalds)',\n",
    " 'CyC2018',\n",
    " 'komeiji-satori (神楽坂覚々)',\n",
    " 'script-8']\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Seth Vargo', 'Danny Mösch', 'ibhagwan', 'Liran Tal', 'yetone', 'Elie Steinbock', 'lauren', 'James Henry', 'Jeremiah Lowin', 'Luis M. Gallardo D.', 'hydai', 'Daniel Öster', 'Karl Seguin', 'Henrik Rydgård', 'Sebastian Raschka', 'comfyanonymous', 'Fatih Arslan', 'AMIT SHEKHAR', 'Travis Cline', \"Paul D'Ambra\", 'Niels Laute', 'Xunzhuo', 'Lucas Gomide', 'Xingchen Song(宋星辰)', 'Lukasz']\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://github.com/trending/developers\"\n",
    "\n",
    "# Fetch page\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# Find all developer entries\n",
    "developers = soup.find_all(\"article\", class_=\"Box-row\")\n",
    "\n",
    "# Extract clean text names (GitHub usernames)\n",
    "dev_names = []\n",
    "for dev in developers:\n",
    "    name_tag = dev.find(\"h1\", class_=\"h3 lh-condensed\")\n",
    "    if name_tag:\n",
    "        username = name_tag.get_text(strip=True)  # removes all HTML tags and extra spaces\n",
    "        dev_names.append(username)\n",
    "\n",
    "print(dev_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Seth Vargo', 'Danny Mösch', 'ibhagwan', 'Liran Tal', 'yetone', 'Elie Steinbock', 'lauren', 'James Henry', 'Jeremiah Lowin', 'Luis M. Gallardo D.', 'hydai', 'Daniel Öster', 'Karl Seguin', 'Henrik Rydgård', 'Sebastian Raschka', 'comfyanonymous', 'Fatih Arslan', 'AMIT SHEKHAR', 'Travis Cline', \"Paul D'Ambra\", 'Niels Laute', 'Xunzhuo', 'Lucas Gomide', 'Xingchen Song(宋星辰)', 'Lukasz']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://github.com/trending/developers\"\n",
    "\n",
    "# Step 1: Fetch page\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# Step 2: Use find_all() with attrs to get the elements containing developer names\n",
    "name_elements = soup.find_all(\"h1\", attrs={\"class\": \"h3 lh-condensed\"})\n",
    "\n",
    "# Step 3: Loop through elements, clean text with .get_text(), strip spaces/linebreaks\n",
    "dev_names = []\n",
    "for elem in name_elements:\n",
    "    clean_name = elem.get_text(separator=\" \", strip=True)  # removes \\n, trims whitespace\n",
    "    dev_names.append(clean_name)\n",
    "\n",
    "# Step 4: Print the list\n",
    "print(dev_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1. Display the trending Python repositories in GitHub.\n",
    "\n",
    "The steps to solve this problem is similar to the previous one except that you need to find out the repository names instead of developer names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://github.com/trending/python?since=daily'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['oraios / serena', 'HKUDS / DeepCode', 'crewAIInc / crewAI', 'murtaza-nasir / maestro', 'QwenLM / Qwen3', 'NVIDIA / Megatron-LM', 'livekit / agents', 'lllyasviel / Fooocus', 'microsoft / rStar', 'resemble-ai / chatterbox', 'denizsafak / abogen', 'huggingface / diffusers', 'JefferyHcool / BiliNote', 'PaddlePaddle / PaddleOCR', 'lfnovo / open-notebook']\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://github.com/trending/python?since=daily\"\n",
    "\n",
    "# Fetch page\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# Step 1: Find all repo name elements\n",
    "repo_elements = soup.find_all(\"h2\", attrs={\"class\": \"h3 lh-condensed\"})\n",
    "\n",
    "# Step 2: Loop through elements, clean text\n",
    "repo_names = []\n",
    "for elem in repo_elements:\n",
    "    # Get text, strip whitespace/newlines, and collapse spaces\n",
    "    clean_name = elem.get_text(separator=\" \", strip=True).replace(\"\\n\", \" \")\n",
    "    repo_names.append(clean_name)\n",
    "\n",
    "# Step 3: Print the list of repositories\n",
    "print(repo_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Display all the image links from Walt Disney wikipedia page.\n",
    "Hint: use `.get()` to access information inside tags. Check out the documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://en.wikipedia.org/wiki/Walt_Disney'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://en.wikipedia.org/static/images/icons/wikipedia.png', 'https://en.wikipedia.org/static/images/mobile/copyright/wikipedia-wordmark-en.svg', 'https://en.wikipedia.org/static/images/mobile/copyright/wikipedia-tagline-en.svg', 'https://upload.wikimedia.org/wikipedia/en/thumb/e/e7/Cscr-featured.svg/20px-Cscr-featured.svg.png', 'https://upload.wikimedia.org/wikipedia/en/thumb/8/8c/Extended-protection-shackle.svg/20px-Extended-protection-shackle.svg.png', 'https://upload.wikimedia.org/wikipedia/commons/thumb/5/50/Walt_Disney_1946_%28cropped2%29.JPG/250px-Walt_Disney_1946_%28cropped2%29.JPG', 'https://upload.wikimedia.org/wikipedia/commons/thumb/8/87/Walt_Disney_1942_signature.svg/250px-Walt_Disney_1942_signature.svg.png', 'https://upload.wikimedia.org/wikipedia/commons/thumb/3/3a/Walt_Disney_Birthplace_Exterior_Hermosa_Chicago_Illinois.jpg/250px-Walt_Disney_Birthplace_Exterior_Hermosa_Chicago_Illinois.jpg', 'https://upload.wikimedia.org/wikipedia/commons/thumb/c/c4/Walt_Disney_envelope_ca._1921.jpg/250px-Walt_Disney_envelope_ca._1921.jpg', 'https://upload.wikimedia.org/wikipedia/commons/thumb/4/43/Walt_Disney_with_film_roll_and_Mickey_Mouse_on_his_right_arm%2C_year_1935.jpg/250px-Walt_Disney_with_film_roll_and_Mickey_Mouse_on_his_right_arm%2C_year_1935.jpg', 'https://upload.wikimedia.org/wikipedia/commons/thumb/c/cd/Walt_Disney_Snow_white_1937_trailer_screenshot_%2813%29.jpg/250px-Walt_Disney_Snow_white_1937_trailer_screenshot_%2813%29.jpg', 'https://upload.wikimedia.org/wikipedia/commons/thumb/1/15/Disney_drawing_goofy.jpg/250px-Disney_drawing_goofy.jpg', 'https://upload.wikimedia.org/wikipedia/commons/thumb/8/8c/WaltDisneyplansDisneylandDec1954.jpg/250px-WaltDisneyplansDisneylandDec1954.jpg', 'https://upload.wikimedia.org/wikipedia/commons/thumb/c/c3/Walt_Disney_and_Dr._Wernher_von_Braun_-_GPN-2000-000060.jpg/250px-Walt_Disney_and_Dr._Wernher_von_Braun_-_GPN-2000-000060.jpg', 'https://upload.wikimedia.org/wikipedia/commons/thumb/7/79/Walt_Disney_with_Company_at_Press_Conference.jpg/250px-Walt_Disney_with_Company_at_Press_Conference.jpg', 'https://upload.wikimedia.org/wikipedia/commons/thumb/1/1a/Walt_Disney_Grave.JPG/250px-Walt_Disney_Grave.JPG', 'https://upload.wikimedia.org/wikipedia/commons/thumb/1/1b/Nuvola_apps_kaboodle.svg/20px-Nuvola_apps_kaboodle.svg.png', 'https://upload.wikimedia.org/wikipedia/commons/thumb/1/13/DisneySchiphol1951.jpg/250px-DisneySchiphol1951.jpg', 'https://upload.wikimedia.org/wikipedia/commons/thumb/6/6c/Disney1968.jpg/250px-Disney1968.jpg', 'https://upload.wikimedia.org/wikipedia/commons/thumb/a/a8/Walt_Disney_Receives_Presidential_Medal_of_Freedom_1964.jpg/330px-Walt_Disney_Receives_Presidential_Medal_of_Freedom_1964.jpg', 'https://upload.wikimedia.org/wikipedia/en/thumb/4/4a/Commons-logo.svg/20px-Commons-logo.svg.png', 'https://upload.wikimedia.org/wikipedia/commons/thumb/f/fa/Wikiquote-logo.svg/40px-Wikiquote-logo.svg.png', 'https://upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Wikisource-logo.svg/40px-Wikisource-logo.svg.png', 'https://upload.wikimedia.org/wikipedia/en/thumb/8/8a/OOjs_UI_icon_edit-ltr-progressive.svg/20px-OOjs_UI_icon_edit-ltr-progressive.svg.png', 'https://upload.wikimedia.org/wikipedia/en/thumb/8/8a/OOjs_UI_icon_edit-ltr-progressive.svg/20px-OOjs_UI_icon_edit-ltr-progressive.svg.png', 'https://upload.wikimedia.org/wikipedia/en/thumb/9/96/Symbol_category_class.svg/20px-Symbol_category_class.svg.png', 'https://upload.wikimedia.org/wikipedia/commons/thumb/e/e3/Disneyland_Resort_logo.svg/250px-Disneyland_Resort_logo.svg.png', 'https://upload.wikimedia.org/wikipedia/commons/thumb/d/da/Animation_disc.svg/20px-Animation_disc.svg.png', 'https://upload.wikimedia.org/wikipedia/en/thumb/6/69/P_vip.svg/20px-P_vip.svg.png', 'https://upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Mickey_Mouse_colored_%28head%29.svg/20px-Mickey_Mouse_colored_%28head%29.svg.png', 'https://upload.wikimedia.org/wikipedia/en/thumb/e/e7/Video-x-generic.svg/20px-Video-x-generic.svg.png', 'https://upload.wikimedia.org/wikipedia/commons/thumb/a/a3/Flag_of_Los_Angeles_County%2C_California.svg/40px-Flag_of_Los_Angeles_County%2C_California.svg.png', 'https://upload.wikimedia.org/wikipedia/commons/thumb/8/8c/Blank_television_set.svg/40px-Blank_television_set.svg.png', 'https://upload.wikimedia.org/wikipedia/en/thumb/a/a4/Flag_of_the_United_States.svg/40px-Flag_of_the_United_States.svg.png', 'https://upload.wikimedia.org/wikipedia/en/thumb/8/8a/OOjs_UI_icon_edit-ltr-progressive.svg/20px-OOjs_UI_icon_edit-ltr-progressive.svg.png', 'https://en.wikipedia.org/wiki/Special:CentralAutoLogin/start?type=1x1&usesul3=1', 'https://en.wikipedia.org/static/images/footer/wikimedia.svg', 'https://en.wikipedia.org/w/resources/assets/mediawiki_compact.svg']\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://en.wikipedia.org/wiki/Walt_Disney\"\n",
    "\n",
    "# Fetch page\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "}\n",
    "response = requests.get(url, headers=headers)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# Step 1: Find all <img> tags\n",
    "img_tags = soup.find_all(\"img\")\n",
    "\n",
    "# Step 2: Extract 'src' attribute using .get()\n",
    "img_links = []\n",
    "for img in img_tags:\n",
    "    src = img.get(\"src\")\n",
    "    if src:  # ensure src exists\n",
    "        # Wikipedia uses relative links, so prepend https: if missing\n",
    "        if src.startswith(\"//\"):\n",
    "            src = \"https:\" + src\n",
    "        elif src.startswith(\"/\"):\n",
    "            src = \"https://en.wikipedia.org\" + src\n",
    "        img_links.append(src)\n",
    "\n",
    "# Step 3: Print the list of image links\n",
    "print(img_links)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1. List all language names and number of related articles in the order they appear in wikipedia.org."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://www.wikipedia.org/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English: 7,050,000+articles\n",
      "æ¥æ¬èª: 1,471,000+è¨äº\n",
      "Ð ÑÑÑÐºÐ¸Ð¹: 2Â 061Â 000+ÑÑÐ°ÑÐµÐ¹\n",
      "Deutsch: 3.046.000+Artikel\n",
      "FranÃ§ais: 2â¯706â¯000+articles\n",
      "EspaÃ±ol: 2.058.000+artÃ­culos\n",
      "ä¸­æ: 1,497,000+æ¡ç® / æ¢ç®\n",
      "Italiano: 1.933.000+voci\n",
      "Polski: 1Â 667Â 000+haseÅ\n",
      "PortuguÃªs: 1.154.000+artigos\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://www.wikipedia.org/\"\n",
    "\n",
    "# Fetch page\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "}\n",
    "response = requests.get(url, headers=headers)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# Step 1: Find all language boxes\n",
    "lang_boxes = soup.find_all(\"a\", attrs={\"class\": \"link-box\"})\n",
    "\n",
    "# Step 2: Extract language name + number of articles\n",
    "languages = []\n",
    "for box in lang_boxes:\n",
    "    lang_name = box.find(\"strong\").get_text(strip=True)\n",
    "    article_count = box.find(\"small\").get_text(strip=True)\n",
    "    languages.append((lang_name, article_count))\n",
    "\n",
    "# Step 3: Print results\n",
    "for lang, count in languages:\n",
    "    print(f\"{lang}: {count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2. Display the top 10 languages by number of native speakers stored in a pandas dataframe.\n",
    "Hint: After finding the correct table you want to analyse, you can use a nested **for** loop to find the elements row by row (check out the 'td' and 'tr' tags). <br>An easier way to do it is using pd.read_html(), check out documentation [here](https://pandas.pydata.org/pandas-docs/version/0.23.4/generated/pandas.read_html.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://en.wikipedia.org/wiki/List_of_languages_by_number_of_native_speakers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Language  Native speakers (millions) Language family        Branch\n",
      "0  Mandarin Chinese                         990    Sino-Tibetan       Sinitic\n",
      "1           Spanish                         484   Indo-European       Romance\n",
      "2           English                         390   Indo-European      Germanic\n",
      "3             Hindi                         345   Indo-European    Indo-Aryan\n",
      "4        Portuguese                         250   Indo-European       Romance\n",
      "5           Bengali                         242   Indo-European    Indo-Aryan\n",
      "6           Russian                         145   Indo-European  Balto-Slavic\n",
      "7          Japanese                         124         Japonic             —\n",
      "8   Western Punjabi                          90   Indo-European    Indo-Aryan\n",
      "9        Vietnamese                          86   Austroasiatic        Vietic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\profe\\AppData\\Local\\Temp\\ipykernel_12300\\3745342812.py:11: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  tables = pd.read_html(response.text)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "url = \"https://en.wikipedia.org/wiki/List_of_languages_by_number_of_native_speakers\"\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "}\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "# Step 1: Read all tables from the page\n",
    "tables = pd.read_html(response.text)\n",
    "\n",
    "# Step 2: The first table on this page is the one we want\n",
    "df = tables[0]\n",
    "\n",
    "# Step 3: Select only the top 10 rows\n",
    "top10 = df.head(10)\n",
    "\n",
    "# Step 4: Display the DataFrame\n",
    "print(top10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Display Metacritic top 24 Best TV Shows of all time (TV Show name, initial release date, metascore rating, film rating system and description) as a pandas dataframe.\n",
    "Hint: If you hover over the title of the movie, you should see the director's name. Can you find where it's stored in the html?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "url = 'https://www.metacritic.com/browse/tv/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Show Name, Initial Release, Metascore, Rating System, Description]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = \"https://www.metacritic.com/browse/tv/\"\n",
    "\n",
    "# Metacritic blocks requests without a User-Agent\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\"\n",
    "}\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# Step 1: Find all shows (each is usually in a div with class 'clamp-summary-wrap')\n",
    "show_divs = soup.find_all(\"div\", class_=\"clamp-summary-wrap\")\n",
    "\n",
    "# Step 2: Extract details\n",
    "data = []\n",
    "for div in show_divs[:24]:  # top 24 shows\n",
    "    show_name_tag = div.find(\"a\", class_=\"title\")\n",
    "    show_name = show_name_tag.get_text(strip=True) if show_name_tag else None\n",
    "\n",
    "    release_tag = div.find(\"div\", class_=\"clamp-details\")\n",
    "    release_date = release_tag.find_all(\"span\")[1].get_text(strip=True) if release_tag else None\n",
    "\n",
    "    metascore_tag = div.find(\"a\", class_=\"metascore_anchor\")\n",
    "    metascore = metascore_tag.find(\"div\").get_text(strip=True) if metascore_tag else None\n",
    "\n",
    "    # Film rating system is sometimes in 'clamp-score-wrap'\n",
    "    rating_tag = div.find(\"div\", class_=\"clamp-score-wrap\")\n",
    "    rating_system = rating_tag.find(\"div\", class_=\"clamp-rating\").get_text(strip=True) if rating_tag and rating_tag.find(\"div\", class_=\"clamp-rating\") else None\n",
    "\n",
    "    # Description / summary\n",
    "    desc_tag = div.find(\"div\", class_=\"summary\")\n",
    "    description = desc_tag.get_text(strip=True) if desc_tag else None\n",
    "\n",
    "    data.append([show_name, release_date, metascore, rating_system, description])\n",
    "\n",
    "# Step 3: Create DataFrame\n",
    "df = pd.DataFrame(data, columns=[\"Show Name\", \"Initial Release\", \"Metascore\", \"Rating System\", \"Description\"])\n",
    "\n",
    "# Step 4: Display\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1. Find the image source link and the TV show link. After you're able to retrieve, add them to your initial dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "# We were told to skip this..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the live weather report (temperature, wind speed, description and weather) of a given city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://openweathermap.org/current\n",
    "city = input('Enter the city: ')\n",
    "url = f'https://api.weatherapi.com/v1/current.json?key=5a68dbd3fe6242678ac130253242505&q={city}&aqi=no'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weather report for berlin:\n",
      "Temperature: 26.1°C\n",
      "Wind Speed: 19.1 kph\n",
      "Description: Sunny\n",
      "Weather icon URL: https://cdn.weatherapi.com/weather/64x64/day/113.png\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "import requests\n",
    "\n",
    "# Step 1: Get city input\n",
    "city = input(\"Enter the city: \")\n",
    "\n",
    "# Step 2: Construct API URL\n",
    "url = f'https://api.weatherapi.com/v1/current.json?key=5a68dbd3fe6242678ac130253242505&q={city}&aqi=no'\n",
    "\n",
    "# Step 3: Fetch data from API\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "}\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "\n",
    "    # Step 4: Extract desired information\n",
    "    temp_c = data['current']['temp_c']\n",
    "    wind_kph = data['current']['wind_kph']\n",
    "    description = data['current']['condition']['text']\n",
    "    weather_icon = data['current']['condition']['icon']\n",
    "\n",
    "    # Step 5: Display results\n",
    "    print(f\"Weather report for {city}:\")\n",
    "    print(f\"Temperature: {temp_c}°C\")\n",
    "    print(f\"Wind Speed: {wind_kph} kph\")\n",
    "    print(f\"Description: {description}\")\n",
    "    print(f\"Weather icon URL: https:{weather_icon}\")\n",
    "\n",
    "else:\n",
    "    print(\"Failed to fetch weather data. Please check the city name or API key.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the book name, price and stock availability from books to scrape website as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise. \n",
    "# It is a fictional bookstore created to be scraped. \n",
    "url = 'http://books.toscrape.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Books Data:\n",
      "================================================================================\n",
      "                                                                                     Book Name  Price Stock Availability\n",
      "                                                                          A Light in the Attic £51.77           In stock\n",
      "                                                                            Tipping the Velvet £53.74           In stock\n",
      "                                                                                    Soumission £50.10           In stock\n",
      "                                                                                 Sharp Objects £47.82           In stock\n",
      "                                                         Sapiens: A Brief History of Humankind £54.23           In stock\n",
      "                                                                               The Requiem Red £22.65           In stock\n",
      "                                            The Dirty Little Secrets of Getting Your Dream Job £33.34           In stock\n",
      "       The Coming Woman: A Novel Based on the Life of the Infamous Feminist, Victoria Woodhull £17.93           In stock\n",
      "The Boys in the Boat: Nine Americans and Their Epic Quest for Gold at the 1936 Berlin Olympics £22.60           In stock\n",
      "                                                                               The Black Maria £52.15           In stock\n",
      "                                                Starving Hearts (Triangular Trade Trilogy, #1) £13.99           In stock\n",
      "                                                                         Shakespeare's Sonnets £20.66           In stock\n",
      "                                                                                   Set Me Free £17.46           In stock\n",
      "                                       Scott Pilgrim's Precious Little Life (Scott Pilgrim #1) £52.29           In stock\n",
      "                                                                     Rip it Up and Start Again £35.02           In stock\n",
      "            Our Band Could Be Your Life: Scenes from the American Indie Underground, 1981-1991 £57.25           In stock\n",
      "                                                                                          Olio £23.88           In stock\n",
      "                                         Mesaerion: The Best Science Fiction Stories 1800-1849 £37.59           In stock\n",
      "                                                                  Libertarianism for Beginners £51.33           In stock\n",
      "                                                                       It's Only the Himalayas £45.17           In stock\n",
      "\n",
      "================================================================================\n",
      "Total books scraped: 20\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_books_to_dataframe():\n",
    "    # URL of the website to scrape\n",
    "    url = 'http://books.toscrape.com/'\n",
    "    \n",
    "    try:\n",
    "        # Send a GET request to the website\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Raise an exception for bad status codes\n",
    "        \n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Find all book containers\n",
    "        books = soup.find_all('article', class_='product_pod')\n",
    "        \n",
    "        # Lists to store the data\n",
    "        book_names = []\n",
    "        prices = []\n",
    "        stock_availabilities = []\n",
    "        \n",
    "        # Extract information for each book\n",
    "        for book in books:\n",
    "            # Extract book name\n",
    "            name = book.h3.a['title']\n",
    "            book_names.append(name)\n",
    "            \n",
    "            # Extract price\n",
    "            price = book.find('p', class_='price_color').text\n",
    "            prices.append(price)\n",
    "            \n",
    "            # Extract stock availability\n",
    "            stock = book.find('p', class_='instock availability').text.strip()\n",
    "            stock_availabilities.append(stock)\n",
    "        \n",
    "        # Create a pandas DataFrame\n",
    "        df = pd.DataFrame({\n",
    "            'Book Name': book_names,\n",
    "            'Price': prices,\n",
    "            'Stock Availability': stock_availabilities\n",
    "        })\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error fetching the webpage: {e}\")\n",
    "        return pd.DataFrame()\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Scrape the books and create DataFrame\n",
    "    books_df = scrape_books_to_dataframe()\n",
    "    \n",
    "    # Check if DataFrame was created successfully\n",
    "    if not books_df.empty:\n",
    "        print(\"Books Data:\")\n",
    "        print(\"=\" * 80)\n",
    "        print(books_df.to_string(index=False))\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(f\"Total books scraped: {len(books_df)}\")\n",
    "    else:\n",
    "        print(\"No data was scraped. Please check the URL or your internet connection.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Display the initial 100 books available in the homepage. Once again, collect the book name, price and its stock availability.\n",
    "\n",
    "***Hint:*** The total number of displayed books per page is 20, but you can easily move to the next page by looping through the desired number of pages and adding it to the end of the url."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://books.toscrape.com/catalogue/page-1.html',\n",
       " 'https://books.toscrape.com/catalogue/page-2.html',\n",
       " 'https://books.toscrape.com/catalogue/page-3.html',\n",
       " 'https://books.toscrape.com/catalogue/page-4.html',\n",
       " 'https://books.toscrape.com/catalogue/page-5.html']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://books.toscrape.com/catalogue/page-'\n",
    "# This is how you will loop through each page:\n",
    "number_of_pages = int(100/20)\n",
    "each_page_urls = []\n",
    "for n in range(1, number_of_pages+1):\n",
    "    link = url+str(n)+\".html\"\n",
    "    each_page_urls.append(link)\n",
    "    \n",
    "each_page_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting web scraping...\n",
      "Scraping page 1: http://books.toscrape.com/\n",
      "Found 20 books on page 1\n",
      "Added book 1: A Light in the Attic\n",
      "Added book 2: Tipping the Velvet\n",
      "Added book 3: Soumission\n",
      "Added book 4: Sharp Objects\n",
      "Added book 5: Sapiens: A Brief History of Humankind\n",
      "Added book 6: The Requiem Red\n",
      "Added book 7: The Dirty Little Secrets of Getting Your Dream Job\n",
      "Added book 8: The Coming Woman: A Novel Based on the Life of the Infamous Feminist, Victoria Woodhull\n",
      "Added book 9: The Boys in the Boat: Nine Americans and Their Epic Quest for Gold at the 1936 Berlin Olympics\n",
      "Added book 10: The Black Maria\n",
      "Added book 11: Starving Hearts (Triangular Trade Trilogy, #1)\n",
      "Added book 12: Shakespeare's Sonnets\n",
      "Added book 13: Set Me Free\n",
      "Added book 14: Scott Pilgrim's Precious Little Life (Scott Pilgrim #1)\n",
      "Added book 15: Rip it Up and Start Again\n",
      "Added book 16: Our Band Could Be Your Life: Scenes from the American Indie Underground, 1981-1991\n",
      "Added book 17: Olio\n",
      "Added book 18: Mesaerion: The Best Science Fiction Stories 1800-1849\n",
      "Added book 19: Libertarianism for Beginners\n",
      "Added book 20: It's Only the Himalayas\n",
      "Scraping page 2: http://books.toscrape.com/catalogue/page-2.html\n",
      "Found 20 books on page 2\n",
      "Added book 21: In Her Wake\n",
      "Added book 22: How Music Works\n",
      "Added book 23: Foolproof Preserving: A Guide to Small Batch Jams, Jellies, Pickles, Condiments, and More: A Foolproof Guide to Making Small Batch Jams, Jellies, Pickles, Condiments, and More\n",
      "Added book 24: Chase Me (Paris Nights #2)\n",
      "Added book 25: Black Dust\n",
      "Added book 26: Birdsong: A Story in Pictures\n",
      "Added book 27: America's Cradle of Quarterbacks: Western Pennsylvania's Football Factory from Johnny Unitas to Joe Montana\n",
      "Added book 28: Aladdin and His Wonderful Lamp\n",
      "Added book 29: Worlds Elsewhere: Journeys Around Shakespeare’s Globe\n",
      "Added book 30: Wall and Piece\n",
      "Added book 31: The Four Agreements: A Practical Guide to Personal Freedom\n",
      "Added book 32: The Five Love Languages: How to Express Heartfelt Commitment to Your Mate\n",
      "Added book 33: The Elephant Tree\n",
      "Added book 34: The Bear and the Piano\n",
      "Added book 35: Sophie's World\n",
      "Added book 36: Penny Maybe\n",
      "Added book 37: Maude (1883-1993):She Grew Up with the country\n",
      "Added book 38: In a Dark, Dark Wood\n",
      "Added book 39: Behind Closed Doors\n",
      "Added book 40: You can't bury them all: Poems\n",
      "Scraping page 3: http://books.toscrape.com/catalogue/page-3.html\n",
      "Found 20 books on page 3\n",
      "Added book 41: Slow States of Collapse: Poems\n",
      "Added book 42: Reasons to Stay Alive\n",
      "Added book 43: Private Paris (Private #10)\n",
      "Added book 44: #HigherSelfie: Wake Up Your Life. Free Your Soul. Find Your Tribe.\n",
      "Added book 45: Without Borders (Wanderlove #1)\n",
      "Added book 46: When We Collided\n",
      "Added book 47: We Love You, Charlie Freeman\n",
      "Added book 48: Untitled Collection: Sabbath Poems 2014\n",
      "Added book 49: Unseen City: The Majesty of Pigeons, the Discreet Charm of Snails & Other Wonders of the Urban Wilderness\n",
      "Added book 50: Unicorn Tracks\n",
      "Added book 51: Unbound: How Eight Technologies Made Us Human, Transformed Society, and Brought Our World to the Brink\n",
      "Added book 52: Tsubasa: WoRLD CHRoNiCLE 2 (Tsubasa WoRLD CHRoNiCLE #2)\n",
      "Added book 53: Throwing Rocks at the Google Bus: How Growth Became the Enemy of Prosperity\n",
      "Added book 54: This One Summer\n",
      "Added book 55: Thirst\n",
      "Added book 56: The Torch Is Passed: A Harding Family Story\n",
      "Added book 57: The Secret of Dreadwillow Carse\n",
      "Added book 58: The Pioneer Woman Cooks: Dinnertime: Comfort Classics, Freezer Food, 16-Minute Meals, and Other Delicious Ways to Solve Supper!\n",
      "Added book 59: The Past Never Ends\n",
      "Added book 60: The Natural History of Us (The Fine Art of Pretending #2)\n",
      "Scraping page 4: http://books.toscrape.com/catalogue/page-4.html\n",
      "Found 20 books on page 4\n",
      "Added book 61: The Nameless City (The Nameless City #1)\n",
      "Added book 62: The Murder That Never Was (Forensic Instincts #5)\n",
      "Added book 63: The Most Perfect Thing: Inside (and Outside) a Bird's Egg\n",
      "Added book 64: The Mindfulness and Acceptance Workbook for Anxiety: A Guide to Breaking Free from Anxiety, Phobias, and Worry Using Acceptance and Commitment Therapy\n",
      "Added book 65: The Life-Changing Magic of Tidying Up: The Japanese Art of Decluttering and Organizing\n",
      "Added book 66: The Inefficiency Assassin: Time Management Tactics for Working Smarter, Not Longer\n",
      "Added book 67: The Gutsy Girl: Escapades for Your Life of Epic Adventure\n",
      "Added book 68: The Electric Pencil: Drawings from Inside State Hospital No. 3\n",
      "Added book 69: The Death of Humanity: and the Case for Life\n",
      "Added book 70: The Bulletproof Diet: Lose up to a Pound a Day, Reclaim Energy and Focus, Upgrade Your Life\n",
      "Added book 71: The Art Forger\n",
      "Added book 72: The Age of Genius: The Seventeenth Century and the Birth of the Modern Mind\n",
      "Added book 73: The Activist's Tao Te Ching: Ancient Advice for a Modern Revolution\n",
      "Added book 74: Spark Joy: An Illustrated Master Class on the Art of Organizing and Tidying Up\n",
      "Added book 75: Soul Reader\n",
      "Added book 76: Security\n",
      "Added book 77: Saga, Volume 6 (Saga (Collected Editions) #6)\n",
      "Added book 78: Saga, Volume 5 (Saga (Collected Editions) #5)\n",
      "Added book 79: Reskilling America: Learning to Labor in the Twenty-First Century\n",
      "Added book 80: Rat Queens, Vol. 3: Demons (Rat Queens (Collected Editions) #11-15)\n",
      "Scraping page 5: http://books.toscrape.com/catalogue/page-5.html\n",
      "Found 20 books on page 5\n",
      "Added book 81: Princess Jellyfish 2-in-1 Omnibus, Vol. 01 (Princess Jellyfish 2-in-1 Omnibus #1)\n",
      "Added book 82: Princess Between Worlds (Wide-Awake Princess #5)\n",
      "Added book 83: Pop Gun War, Volume 1: Gift\n",
      "Added book 84: Political Suicide: Missteps, Peccadilloes, Bad Calls, Backroom Hijinx, Sordid Pasts, Rotten Breaks, and Just Plain Dumb Mistakes in the Annals of American Politics\n",
      "Added book 85: Patience\n",
      "Added book 86: Outcast, Vol. 1: A Darkness Surrounds Him (Outcast #1)\n",
      "Added book 87: orange: The Complete Collection 1 (orange: The Complete Collection #1)\n",
      "Added book 88: Online Marketing for Busy Authors: A Step-By-Step Guide\n",
      "Added book 89: On a Midnight Clear\n",
      "Added book 90: Obsidian (Lux #1)\n",
      "Added book 91: My Paris Kitchen: Recipes and Stories\n",
      "Added book 92: Masks and Shadows\n",
      "Added book 93: Mama Tried: Traditional Italian Cooking for the Screwed, Crude, Vegan, and Tattooed\n",
      "Added book 94: Lumberjanes, Vol. 2: Friendship to the Max (Lumberjanes #5-8)\n",
      "Added book 95: Lumberjanes, Vol. 1: Beware the Kitten Holy (Lumberjanes #1-4)\n",
      "Added book 96: Lumberjanes Vol. 3: A Terrible Plan (Lumberjanes #9-12)\n",
      "Added book 97: Layered: Baking, Building, and Styling Spectacular Cakes\n",
      "Added book 98: Judo: Seven Steps to Black Belt (an Introductory Guide for Beginners)\n",
      "Added book 99: Join\n",
      "Added book 100: In the Country We Love: My Family Divided\n",
      "\n",
      "====================================================================================================\n",
      "BOOKS DATA:\n",
      "====================================================================================================\n",
      "                                                                                                                                                                      Book Name  Price Stock Availability\n",
      "                                                                                                                                                           A Light in the Attic £51.77           In stock\n",
      "                                                                                                                                                             Tipping the Velvet £53.74           In stock\n",
      "                                                                                                                                                                     Soumission £50.10           In stock\n",
      "                                                                                                                                                                  Sharp Objects £47.82           In stock\n",
      "                                                                                                                                          Sapiens: A Brief History of Humankind £54.23           In stock\n",
      "                                                                                                                                                                The Requiem Red £22.65           In stock\n",
      "                                                                                                                             The Dirty Little Secrets of Getting Your Dream Job £33.34           In stock\n",
      "                                                                                        The Coming Woman: A Novel Based on the Life of the Infamous Feminist, Victoria Woodhull £17.93           In stock\n",
      "                                                                                 The Boys in the Boat: Nine Americans and Their Epic Quest for Gold at the 1936 Berlin Olympics £22.60           In stock\n",
      "                                                                                                                                                                The Black Maria £52.15           In stock\n",
      "                                                                                                                                 Starving Hearts (Triangular Trade Trilogy, #1) £13.99           In stock\n",
      "                                                                                                                                                          Shakespeare's Sonnets £20.66           In stock\n",
      "                                                                                                                                                                    Set Me Free £17.46           In stock\n",
      "                                                                                                                        Scott Pilgrim's Precious Little Life (Scott Pilgrim #1) £52.29           In stock\n",
      "                                                                                                                                                      Rip it Up and Start Again £35.02           In stock\n",
      "                                                                                             Our Band Could Be Your Life: Scenes from the American Indie Underground, 1981-1991 £57.25           In stock\n",
      "                                                                                                                                                                           Olio £23.88           In stock\n",
      "                                                                                                                          Mesaerion: The Best Science Fiction Stories 1800-1849 £37.59           In stock\n",
      "                                                                                                                                                   Libertarianism for Beginners £51.33           In stock\n",
      "                                                                                                                                                        It's Only the Himalayas £45.17           In stock\n",
      "                                                                                                                                                                    In Her Wake £12.84           In stock\n",
      "                                                                                                                                                                How Music Works £37.32           In stock\n",
      "Foolproof Preserving: A Guide to Small Batch Jams, Jellies, Pickles, Condiments, and More: A Foolproof Guide to Making Small Batch Jams, Jellies, Pickles, Condiments, and More £30.52           In stock\n",
      "                                                                                                                                                     Chase Me (Paris Nights #2) £25.27           In stock\n",
      "                                                                                                                                                                     Black Dust £34.53           In stock\n",
      "                                                                                                                                                  Birdsong: A Story in Pictures £54.64           In stock\n",
      "                                                                    America's Cradle of Quarterbacks: Western Pennsylvania's Football Factory from Johnny Unitas to Joe Montana £22.50           In stock\n",
      "                                                                                                                                                 Aladdin and His Wonderful Lamp £53.13           In stock\n",
      "                                                                                                                          Worlds Elsewhere: Journeys Around Shakespeare’s Globe £40.30           In stock\n",
      "                                                                                                                                                                 Wall and Piece £44.18           In stock\n",
      "                                                                                                                     The Four Agreements: A Practical Guide to Personal Freedom £17.66           In stock\n",
      "                                                                                                      The Five Love Languages: How to Express Heartfelt Commitment to Your Mate £31.05           In stock\n",
      "                                                                                                                                                              The Elephant Tree £23.82           In stock\n",
      "                                                                                                                                                         The Bear and the Piano £36.89           In stock\n",
      "                                                                                                                                                                 Sophie's World £15.94           In stock\n",
      "                                                                                                                                                                    Penny Maybe £33.29           In stock\n",
      "                                                                                                                                 Maude (1883-1993):She Grew Up with the country £18.02           In stock\n",
      "                                                                                                                                                           In a Dark, Dark Wood £19.63           In stock\n",
      "                                                                                                                                                            Behind Closed Doors £52.22           In stock\n",
      "                                                                                                                                                 You can't bury them all: Poems £33.63           In stock\n",
      "                                                                                                                                                 Slow States of Collapse: Poems £57.31           In stock\n",
      "                                                                                                                                                          Reasons to Stay Alive £26.41           In stock\n",
      "                                                                                                                                                    Private Paris (Private #10) £47.61           In stock\n",
      "                                                                                                             #HigherSelfie: Wake Up Your Life. Free Your Soul. Find Your Tribe. £23.11           In stock\n",
      "                                                                                                                                                Without Borders (Wanderlove #1) £45.07           In stock\n",
      "                                                                                                                                                               When We Collided £31.77           In stock\n",
      "                                                                                                                                                   We Love You, Charlie Freeman £50.27           In stock\n",
      "                                                                                                                                        Untitled Collection: Sabbath Poems 2014 £14.27           In stock\n",
      "                                                                      Unseen City: The Majesty of Pigeons, the Discreet Charm of Snails & Other Wonders of the Urban Wilderness £44.18           In stock\n",
      "                                                                                                                                                                 Unicorn Tracks £18.78           In stock\n",
      "                                                                         Unbound: How Eight Technologies Made Us Human, Transformed Society, and Brought Our World to the Brink £25.52           In stock\n",
      "                                                                                                                        Tsubasa: WoRLD CHRoNiCLE 2 (Tsubasa WoRLD CHRoNiCLE #2) £16.28           In stock\n",
      "                                                                                                    Throwing Rocks at the Google Bus: How Growth Became the Enemy of Prosperity £31.12           In stock\n",
      "                                                                                                                                                                This One Summer £19.49           In stock\n",
      "                                                                                                                                                                         Thirst £17.27           In stock\n",
      "                                                                                                                                    The Torch Is Passed: A Harding Family Story £19.09           In stock\n",
      "                                                                                                                                                The Secret of Dreadwillow Carse £56.13           In stock\n",
      "                                                The Pioneer Woman Cooks: Dinnertime: Comfort Classics, Freezer Food, 16-Minute Meals, and Other Delicious Ways to Solve Supper! £56.41           In stock\n",
      "                                                                                                                                                            The Past Never Ends £56.50           In stock\n",
      "                                                                                                                      The Natural History of Us (The Fine Art of Pretending #2) £45.22           In stock\n",
      "                                                                                                                                       The Nameless City (The Nameless City #1) £38.16           In stock\n",
      "                                                                                                                              The Murder That Never Was (Forensic Instincts #5) £54.11           In stock\n",
      "                                                                                                                      The Most Perfect Thing: Inside (and Outside) a Bird's Egg £42.96           In stock\n",
      "                         The Mindfulness and Acceptance Workbook for Anxiety: A Guide to Breaking Free from Anxiety, Phobias, and Worry Using Acceptance and Commitment Therapy £23.89           In stock\n",
      "                                                                                         The Life-Changing Magic of Tidying Up: The Japanese Art of Decluttering and Organizing £16.77           In stock\n",
      "                                                                                             The Inefficiency Assassin: Time Management Tactics for Working Smarter, Not Longer £20.59           In stock\n",
      "                                                                                                                      The Gutsy Girl: Escapades for Your Life of Epic Adventure £37.13           In stock\n",
      "                                                                                                                 The Electric Pencil: Drawings from Inside State Hospital No. 3 £56.06           In stock\n",
      "                                                                                                                                   The Death of Humanity: and the Case for Life £58.11           In stock\n",
      "                                                                                    The Bulletproof Diet: Lose up to a Pound a Day, Reclaim Energy and Focus, Upgrade Your Life £49.05           In stock\n",
      "                                                                                                                                                                 The Art Forger £40.76           In stock\n",
      "                                                                                                    The Age of Genius: The Seventeenth Century and the Birth of the Modern Mind £19.73           In stock\n",
      "                                                                                                            The Activist's Tao Te Ching: Ancient Advice for a Modern Revolution £32.24           In stock\n",
      "                                                                                                 Spark Joy: An Illustrated Master Class on the Art of Organizing and Tidying Up £41.83           In stock\n",
      "                                                                                                                                                                    Soul Reader £39.58           In stock\n",
      "                                                                                                                                                                       Security £39.25           In stock\n",
      "                                                                                                                                  Saga, Volume 6 (Saga (Collected Editions) #6) £25.02           In stock\n",
      "                                                                                                                                  Saga, Volume 5 (Saga (Collected Editions) #5) £51.04           In stock\n",
      "                                                                                                              Reskilling America: Learning to Labor in the Twenty-First Century £19.83           In stock\n",
      "                                                                                                            Rat Queens, Vol. 3: Demons (Rat Queens (Collected Editions) #11-15) £50.40           In stock\n",
      "                                                                                              Princess Jellyfish 2-in-1 Omnibus, Vol. 01 (Princess Jellyfish 2-in-1 Omnibus #1) £13.61           In stock\n",
      "                                                                                                                               Princess Between Worlds (Wide-Awake Princess #5) £13.34           In stock\n",
      "                                                                                                                                                    Pop Gun War, Volume 1: Gift £18.97           In stock\n",
      "            Political Suicide: Missteps, Peccadilloes, Bad Calls, Backroom Hijinx, Sordid Pasts, Rotten Breaks, and Just Plain Dumb Mistakes in the Annals of American Politics £36.28           In stock\n",
      "                                                                                                                                                                       Patience £10.16           In stock\n",
      "                                                                                                                         Outcast, Vol. 1: A Darkness Surrounds Him (Outcast #1) £15.44           In stock\n",
      "                                                                                                         orange: The Complete Collection 1 (orange: The Complete Collection #1) £48.41           In stock\n",
      "                                                                                                                        Online Marketing for Busy Authors: A Step-By-Step Guide £46.35           In stock\n",
      "                                                                                                                                                            On a Midnight Clear £14.07           In stock\n",
      "                                                                                                                                                              Obsidian (Lux #1) £14.86           In stock\n",
      "                                                                                                                                          My Paris Kitchen: Recipes and Stories £33.37           In stock\n",
      "                                                                                                                                                              Masks and Shadows £56.40           In stock\n",
      "                                                                                            Mama Tried: Traditional Italian Cooking for the Screwed, Crude, Vegan, and Tattooed £14.02           In stock\n",
      "                                                                                                                  Lumberjanes, Vol. 2: Friendship to the Max (Lumberjanes #5-8) £46.91           In stock\n",
      "                                                                                                                 Lumberjanes, Vol. 1: Beware the Kitten Holy (Lumberjanes #1-4) £45.61           In stock\n",
      "                                                                                                                        Lumberjanes Vol. 3: A Terrible Plan (Lumberjanes #9-12) £19.92           In stock\n",
      "                                                                                                                       Layered: Baking, Building, and Styling Spectacular Cakes £40.11           In stock\n",
      "                                                                                                          Judo: Seven Steps to Black Belt (an Introductory Guide for Beginners) £53.90           In stock\n",
      "                                                                                                                                                                           Join £35.67           In stock\n",
      "                                                                                                                                      In the Country We Love: My Family Divided £22.00           In stock\n",
      "\n",
      "====================================================================================================\n",
      "Total books scraped: 100\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_100_books():\n",
    "    base_url = 'http://books.toscrape.com/'\n",
    "    books_data = []\n",
    "    page_number = 1\n",
    "    \n",
    "    try:\n",
    "        while len(books_data) < 100:\n",
    "            # Construct URL for current page\n",
    "            if page_number == 1:\n",
    "                url = base_url\n",
    "            else:\n",
    "                url = f\"{base_url}catalogue/page-{page_number}.html\"\n",
    "            \n",
    "            print(f\"Scraping page {page_number}: {url}\")\n",
    "            \n",
    "            # Send GET request with headers to avoid blocking\n",
    "            headers = {\n",
    "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "            }\n",
    "            \n",
    "            response = requests.get(url, headers=headers, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            # Parse HTML content\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            # Find all book containers\n",
    "            books = soup.find_all('article', class_='product_pod')\n",
    "            \n",
    "            print(f\"Found {len(books)} books on page {page_number}\")\n",
    "            \n",
    "            # If no books found, break the loop\n",
    "            if not books:\n",
    "                print(\"No more books found. Stopping.\")\n",
    "                break\n",
    "            \n",
    "            # Extract information for each book\n",
    "            for book in books:\n",
    "                if len(books_data) >= 100:\n",
    "                    break\n",
    "                \n",
    "                try:\n",
    "                    # Extract book name\n",
    "                    name = book.h3.a['title']\n",
    "                    \n",
    "                    # Extract price\n",
    "                    price_element = book.find('p', class_='price_color')\n",
    "                    price = price_element.text if price_element else 'Price not found'\n",
    "                    \n",
    "                    # Extract stock availability\n",
    "                    stock_element = book.find('p', class_='instock')\n",
    "                    if not stock_element:\n",
    "                        stock_element = book.find('p', class_='availability')\n",
    "                    stock = stock_element.text.strip() if stock_element else 'Stock not found'\n",
    "                    \n",
    "                    books_data.append({\n",
    "                        'Book Name': name,\n",
    "                        'Price': price,\n",
    "                        'Stock Availability': stock\n",
    "                    })\n",
    "                    \n",
    "                    print(f\"Added book {len(books_data)}: {name}\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing a book: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            # Check if there's a next page\n",
    "            next_button = soup.find('li', class_='next')\n",
    "            if not next_button and len(books_data) < 100:\n",
    "                print(\"No more pages available. Stopping.\")\n",
    "                break\n",
    "                \n",
    "            page_number += 1\n",
    "            \n",
    "        # Create pandas DataFrame\n",
    "        df = pd.DataFrame(books_data)\n",
    "        return df\n",
    "        \n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error fetching the webpage: {e}\")\n",
    "        return pd.DataFrame()\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Alternative simpler approach for just the first page (20 books)\n",
    "def scrape_first_page():\n",
    "    url = 'http://books.toscrape.com/'\n",
    "    \n",
    "    try:\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "        }\n",
    "        \n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        books = soup.find_all('article', class_='product_pod')\n",
    "        \n",
    "        books_data = []\n",
    "        for book in books:\n",
    "            name = book.h3.a['title']\n",
    "            price = book.find('p', class_='price_color').text\n",
    "            stock = book.find('p', class_='instock availability').text.strip()\n",
    "            \n",
    "            books_data.append({\n",
    "                'Book Name': name,\n",
    "                'Price': price,\n",
    "                'Stock Availability': stock\n",
    "            })\n",
    "        \n",
    "        return pd.DataFrame(books_data)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting web scraping...\")\n",
    "    \n",
    "    # Try the full 100 books approach first\n",
    "    books_df = scrape_100_books()\n",
    "    \n",
    "    # If that fails, try just the first page\n",
    "    if books_df.empty:\n",
    "        print(\"\\nTrying to scrape just the first page...\")\n",
    "        books_df = scrape_first_page()\n",
    "    \n",
    "    # Display results\n",
    "    if not books_df.empty:\n",
    "        print(\"\\n\" + \"=\" * 100)\n",
    "        print(\"BOOKS DATA:\")\n",
    "        print(\"=\" * 100)\n",
    "        \n",
    "        # Set display options for better formatting\n",
    "        pd.set_option('display.max_rows', None)\n",
    "        pd.set_option('display.max_columns', None)\n",
    "        pd.set_option('display.width', None)\n",
    "        pd.set_option('display.max_colwidth', 40)\n",
    "        \n",
    "        print(books_df.to_string(index=False))\n",
    "        print(\"\\n\" + \"=\" * 100)\n",
    "        print(f\"Total books scraped: {len(books_df)}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"Failed to scrape any data. Possible reasons:\")\n",
    "        print(\"1. Website might be down or blocking requests\")\n",
    "        print(\"2. Internet connection issue\")\n",
    "        print(\"3. Website structure might have changed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
